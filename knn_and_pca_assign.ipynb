{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**KNN AND PCA ASSIGNMENT**"
      ],
      "metadata": {
        "id": "hnlJ0v3bp0YW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?\n",
        "\n",
        "Ans1. K-Nearest Neighbors (KNN) is a simple supervised machine learning algorithm that makes predictions based on the closest data points in the training set.\n",
        "\n",
        "It stores all training examples and, for a new input, finds the K nearest neighbors using a distance metric like Euclidean distance.\n",
        "\n",
        "In classification, it predicts the class that most of the K neighbors belong to (majority vote).\n",
        "\n",
        "In regression, it predicts the average of the neighbors’ values.\n",
        "\n",
        "Question 2: What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?\n",
        "\n",
        "Ans2. The Curse of Dimensionality refers to problems that arise when your data has a very large number of features (dimensions). As dimensions increase, the space grows so fast that data points become extremely sparse and distances between them become less meaningful. This makes it harder for models to find real patterns in the data.\n",
        "\n",
        "How it affects KNN performance:\n",
        "\n",
        "Distance loses meaning: In high dimensions, distances between points tend to become similar, so KNN cannot reliably tell which points are truly “nearest.”\n",
        "\n",
        "Degraded accuracy: Since KNN depends on distances to find neighbors, its ability to correctly classify or predict drops.\n",
        "\n",
        "More computation: Calculating distances in many dimensions requires more time and resources.\n",
        "\n",
        "Question 3: What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?\n",
        "\n",
        "Ans3. Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms high-dimensional data into a new set of principal components — these are linear combinations of the original features that capture the maximum variance in the data. It projects the data onto these new axes so that most of the important information is retained with fewer dimensions. PCA is a feature transformation/extraction method, whereas feature selection actually selects which original variables to keep.\n",
        "\n",
        "Question 4: What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?\n",
        "\n",
        "Ans4. Eigenvectors are the directions (new axes) along which the data varies the most. Each eigenvector defines a principal component — a direction in the transformed feature space where the data has meaningful structure.\n",
        " Eigenvalues are the scalars that tell you how much variance in the data is captured along each eigenvector direction. A larger eigenvalue means that its corresponding eigenvector explains more of the data’s variability.\n",
        " You rank principal components by their eigenvalues — highest to lowest — because those with larger eigenvalues capture the most significant patterns in the data.\n",
        "\n",
        "By selecting top eigenvectors (with largest eigenvalues), PCA reduces dimensionality while retaining most of the original variance.\n",
        "\n",
        "This process lets you simplify data and remove noise or redundant dimensions with minimal information loss.\n",
        "\n",
        "Question 5: How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?\n",
        "\n",
        "Ans5. PCA first reduces dimensionality by transforming the data into a smaller set of principal components, keeping most of the important information while removing noise and redundant features. This helps mitigate the curse of dimensionality that can weaken KNN’s distance-based predictions.\n",
        " KNN then runs on the PCA-transformed data, where distances are more meaningful and computations are faster because there are fewer features to compare. This can improve KNN’s accuracy and efficiency compared to using all original high-dimensional features.\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "a6N6xoiWqC1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 6: Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases.\n",
        "#ans6.\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# load data\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# without scaling\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(X_train, y_train)\n",
        "print(\"Accuracy without scaling:\", accuracy_score(y_test, knn.predict(X_test)))\n",
        "\n",
        "# with scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_train_s, X_test_s, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(X_train_s, y_train)\n",
        "print(\"Accuracy with scaling:\", accuracy_score(y_test, knn.predict(X_test_s)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CK9Dr8GsmY0",
        "outputId": "db19eebb-995a-4cc6-c824-345760311164"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.7407407407407407\n",
            "Accuracy with scaling: 0.9629629629629629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 7: Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component.\n",
        "#Ans7.\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# 1. load wine data\n",
        "wine = load_wine()\n",
        "X = wine.data  # 178 samples, 13 features :contentReference[oaicite:0]{index=0}\n",
        "\n",
        "# 2. standardize features (important for PCA)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 3. fit PCA with all components\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# 4. print explained variance ratio\n",
        "print(\"Explained variance ratio for each principal component:\")\n",
        "for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
        "    print(f\"PC{i+1}: {ratio:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mEjCqMmtLnp",
        "outputId": "95de3d2f-969d-4053-e2bb-e69ec51c5118"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained variance ratio for each principal component:\n",
            "PC1: 0.3620\n",
            "PC2: 0.1921\n",
            "PC3: 0.1112\n",
            "PC4: 0.0707\n",
            "PC5: 0.0656\n",
            "PC6: 0.0494\n",
            "PC7: 0.0424\n",
            "PC8: 0.0268\n",
            "PC9: 0.0222\n",
            "PC10: 0.0193\n",
            "PC11: 0.0174\n",
            "PC12: 0.0130\n",
            "PC13: 0.0080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset.\n",
        "#Ans8.\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# load wine data\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# split raw data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# scale raw features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# train KNN on original scaled data\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "orig_acc = accuracy_score(y_test, knn.predict(X_test_scaled))\n",
        "\n",
        "# PCA reduce to top 2 components\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# train KNN on PCA data\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "pca_acc = accuracy_score(y_test, knn_pca.predict(X_test_pca))\n",
        "\n",
        "print(\"Accuracy (original):\", orig_acc)\n",
        "print(\"Accuracy (PCA 2 components):\", pca_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6GRBqkntXVw",
        "outputId": "68981588-2304-4433-8e2b-8cb142973289"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (original): 0.9629629629629629\n",
            "Accuracy (PCA 2 components): 0.9814814814814815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 9: Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results.\n",
        "#Ans9.\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# load data\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# scale features (important for fair comparison)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# KNN with Euclidean distance\n",
        "knn_euc = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euc.fit(X_train, y_train)\n",
        "acc_euc = accuracy_score(y_test, knn_euc.predict(X_test))\n",
        "\n",
        "# KNN with Manhattan distance\n",
        "knn_man = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_man.fit(X_train, y_train)\n",
        "acc_man = accuracy_score(y_test, knn_man.predict(X_test))\n",
        "\n",
        "print(\"Euclidean Distance Accuracy:\", acc_euc)\n",
        "print(\"Manhattan Distance Accuracy:\", acc_man)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcEMU0yRtkeN",
        "outputId": "63469ff7-8443-4528-ac33-602ef0456c04"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Euclidean Distance Accuracy: 0.9629629629629629\n",
            "Manhattan Distance Accuracy: 0.9629629629629629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 10: You are working with a high-dimensional gene expression dataset to classify patients with different types of cancer. Due to the large number of features and a small number of samples, traditional models overfit.\n",
        "#Explain how you would:\n",
        "#● Use PCA to reduce dimensionality\n",
        "#● Decide how many components to keep\n",
        "#● Use KNN for classification post-dimensionality reduction\n",
        "#● Evaluate the model\n",
        "#● Justify this pipeline to your stakeholders as a robust solution for real-world biomedical data\n",
        "#Ans10.\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_wine  # use wine dataset as stand-in for gene data\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# --- 1) Load and scale the dataset ---\n",
        "X, y = load_wine(return_X_y=True)  # substitute with your gene expression data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# --- 2) Train/test split ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# --- 3) Baseline KNN (no PCA) ---\n",
        "knn_baseline = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_baseline.fit(X_train, y_train)\n",
        "y_pred_base = knn_baseline.predict(X_test)\n",
        "\n",
        "print(\"=== Baseline KNN (No PCA) ===\")\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred_base))\n",
        "print(classification_report(y_test, y_pred_base))\n",
        "\n",
        "# --- 4) PCA dimensionality reduction ---\n",
        "pca = PCA()\n",
        "pca.fit(X_train)\n",
        "\n",
        "# show explained variance ratio\n",
        "print(\"\\nExplained Variance Ratio (all components):\")\n",
        "for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
        "    print(f\"PC{i+1}: {ratio:.4f}\")\n",
        "\n",
        "# choose number of components to retain e.g., 95% variance\n",
        "cum_var = np.cumsum(pca.explained_variance_ratio_)\n",
        "n_components_95 = np.argmax(cum_var >= 0.95) + 1\n",
        "print(f\"\\n# components for ~95% variance: {n_components_95}\")\n",
        "\n",
        "# apply PCA with top components\n",
        "pca = PCA(n_components=n_components_95)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "# --- 5) KNN on PCA features ---\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "\n",
        "print(\"\\n=== KNN after PCA ===\")\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred_pca))\n",
        "print(classification_report(y_test, y_pred_pca))\n",
        "\n",
        "# --- 6) Optional: cross-validation comparison ---\n",
        "cv_scores_base = cross_val_score(knn_baseline, X_scaled, y, cv=5, scoring='accuracy')\n",
        "cv_scores_pca = cross_val_score(knn_pca, pca.transform(X_scaled), y, cv=5, scoring='accuracy')\n",
        "\n",
        "print(\"\\nCross-val Accuracies (no PCA):\", cv_scores_base.mean())\n",
        "print(\"Cross-val Accuracies (with PCA):\", cv_scores_pca.mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1IYVbiOuGs3",
        "outputId": "3ea2f26f-2a9b-4bbf-a4f2-92218cad408d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Baseline KNN (No PCA) ===\n",
            "Test Accuracy: 0.9444444444444444\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        18\n",
            "           1       1.00      0.86      0.92        21\n",
            "           2       0.83      1.00      0.91        15\n",
            "\n",
            "    accuracy                           0.94        54\n",
            "   macro avg       0.94      0.95      0.94        54\n",
            "weighted avg       0.95      0.94      0.94        54\n",
            "\n",
            "\n",
            "Explained Variance Ratio (all components):\n",
            "PC1: 0.3498\n",
            "PC2: 0.1972\n",
            "PC3: 0.1108\n",
            "PC4: 0.0777\n",
            "PC5: 0.0691\n",
            "PC6: 0.0525\n",
            "PC7: 0.0438\n",
            "PC8: 0.0247\n",
            "PC9: 0.0203\n",
            "PC10: 0.0193\n",
            "PC11: 0.0168\n",
            "PC12: 0.0110\n",
            "PC13: 0.0069\n",
            "\n",
            "# components for ~95% variance: 10\n",
            "\n",
            "=== KNN after PCA ===\n",
            "Test Accuracy: 0.9444444444444444\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        18\n",
            "           1       1.00      0.86      0.92        21\n",
            "           2       0.83      1.00      0.91        15\n",
            "\n",
            "    accuracy                           0.94        54\n",
            "   macro avg       0.94      0.95      0.94        54\n",
            "weighted avg       0.95      0.94      0.94        54\n",
            "\n",
            "\n",
            "Cross-val Accuracies (no PCA): 0.9550793650793651\n",
            "Cross-val Accuracies (with PCA): 0.9493650793650794\n"
          ]
        }
      ]
    }
  ]
}